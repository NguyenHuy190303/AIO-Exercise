{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa2b6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip cache purge\n",
    "# ! pip install --user albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e7e1d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T11:49:37.271610Z",
     "iopub.status.busy": "2025-01-19T11:49:37.271354Z",
     "iopub.status.idle": "2025-01-19T11:49:46.534230Z",
     "shell.execute_reply": "2025-01-19T11:49:46.533497Z"
    },
    "papermill": {
     "duration": 9.269489,
     "end_time": "2025-01-19T11:49:46.535859",
     "exception": false,
     "start_time": "2025-01-19T11:49:37.266370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "# from termcolor import colored\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2468fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.datasets import VOCDetection\n",
    "\n",
    "# dataset = VOCDetection(\n",
    "#     root=\"path_to_save_voc\",  # Đường dẫn thư mục lưu trữ dataset\n",
    "#     year=\"2007\",             # VOC 2007 hoặc 2012\n",
    "#     image_set=\"train\",        # 'train', 'val', hoặc 'trainval'\n",
    "#     download=True             # Tải dataset nếu chưa có\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08ca491c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T11:49:46.544508Z",
     "iopub.status.busy": "2025-01-19T11:49:46.544108Z",
     "iopub.status.idle": "2025-01-19T11:49:46.556031Z",
     "shell.execute_reply": "2025-01-19T11:49:46.555416Z"
    },
    "papermill": {
     "duration": 0.017174,
     "end_time": "2025-01-19T11:49:46.557127",
     "exception": false,
     "start_time": "2025-01-19T11:49:46.539953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "class CustomVOCDataset(torchvision.datasets.VOCDetection):\n",
    "    def __init__(self, root, year='2007', image_set='train', download=False,\n",
    "                 class_mapping=None, s=7, b=2, c=20, custom_transforms=None):\n",
    "        # Gọi hàm khởi tạo của VOCDetection\n",
    "        super().__init__(root, year, image_set, download)\n",
    "        self.s = s  # Grid size sxs\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.class_mapping = class_mapping  # Mapping class name to class indices\n",
    "        self.custom_transforms = custom_transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, target = super(CustomVOCDataset, self).__getitem__(index)\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        boxes = self.convert_to_yolo_format(target, class_mapping)\n",
    "        bboxes = boxes[:, 1:]\n",
    "        labels = boxes[:, 0]\n",
    "\n",
    "        ### Augentation\n",
    "\n",
    "        if self.custom_transforms :\n",
    "            augmented = self.custom_transforms()(\n",
    "                image=np.array(image),\n",
    "                bboxes=bboxes,\n",
    "                labels=labels\n",
    "                                                 )\n",
    "\n",
    "            image = augmented['image'] # np.array\n",
    "            bboxes = augmented['bboxes'] # list \n",
    "            labels = augmented['labels'] # list\n",
    "\n",
    "\n",
    "        # as_tensor để tiết kiệm bộ nhớ và nhanh hơn\n",
    "        image = torch.as_tensor(image)\n",
    "        bboxes = torch.tensor(bboxes) # bbox đã tự chuẩn hóa về 0 - 1\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "\n",
    "        #  vì mình muốn 1 ô chỉ vẽ 1 bbox thôi nên B = 1 => ()\n",
    "        labels_matrix = torch.zeros(self.s, self.s, (5 * self.b) + self.c)\n",
    "        \n",
    "        # Iterate through each bounding box in YOLO format .\n",
    "        for bbox, label in zip(bboxes, labels):\n",
    "            x, y, width, height = bbox.tolist()\n",
    "            class_label = int(label)\n",
    "            \n",
    "            # calculate the grid cell (i, j) the box belongs to\n",
    "            i, j = int(self.s * y), int(self.s * x) # 7 * 0.53 = 3 (index đánh từ 0)\n",
    "            x_cell, y_cell = self.s * x - j, self.s * y - i  # 7×0.53−3=0.71 : vị trí tương đối x, y nằm trong cell đó\n",
    "\n",
    "            # tỷ lệ w, h của bbox ánh xạ sang w, h trong cell  : width_cell=0.2×7=1.4 nghĩa là  chiều rộng 1.4 lần chiều rộng của một ô lưới.\n",
    "            width_cell, height_cell = width * self.s, height * self.s\n",
    "\n",
    "\n",
    "            # If no object has been found in this specific cell (i, j) before:\n",
    "            if labels_matrix[i, j, 20] == 0:\n",
    "                # Mark that an object exists in this cell.\n",
    "                labels_matrix[i, j, 20] = 1\n",
    "\n",
    "                # Store the box coordinates as an offset from the cell boundaries.\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                # Set the box coordinates in the label matrix.\n",
    "                labels_matrix[i, j, 21:25] = box_coordinates\n",
    "\n",
    "                # Set the one-hot encoding for the class label.\n",
    "                labels_matrix[i, j, class_label] = 1\n",
    "\n",
    "        # labels_matrix ở datasets chỉ gắn 25 ô thôi (vì 1 ô tối đa chỉ có 1 class)\n",
    "        # còn khi qua trainning thì gắn đủ 30 ô vì 1 ô được đoán 2 class\n",
    "        return image, labels_matrix\n",
    "\n",
    "\n",
    "\n",
    "    def convert_to_yolo_format(self, target, class_mapping):\n",
    "        \"\"\"\n",
    "            Convert annotation data from VOC format to YOLO format.\n",
    "\n",
    "            Parameters:\n",
    "            target (dict): Annotation data from VOCDetection dataset.\n",
    "            img_width (int): Width of the original image.\n",
    "            img_height (int): Height of the original image.\n",
    "            class_mapping (dict): Mapping from class names to integer IDs.\n",
    "\n",
    "            Returns:\n",
    "            torch.Tensor: Tensor of shape [N, 5] for N bounding boxes,\n",
    "            each with [class_id, x_center, y_center, width, height].\n",
    "        \"\"\"\n",
    "\n",
    "        annotations = target['annotation']['object']\n",
    "        real_width, real_height = int(target['annotation']['size']['width']), int(\n",
    "            target['annotation']['size']['height'])\n",
    "\n",
    "        boxes = []\n",
    "\n",
    "        # Loop through each annotation and convert it to YOLO format.\n",
    "        for anno in annotations:\n",
    "            xmin = int(anno['bndbox']['xmin']) / real_width\n",
    "            xmax = int(anno['bndbox']['xmax']) / real_width\n",
    "            ymin = int(anno['bndbox']['ymin']) / real_height\n",
    "            ymax = int(anno['bndbox']['ymax']) / real_height\n",
    "\n",
    "            x_center = (xmin + xmax) / 2\n",
    "            y_center = (ymin + ymax) / 2\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "            class_name = anno['name']\n",
    "            class_id = class_mapping[class_name] if class_name in class_mapping else 0\n",
    "\n",
    "            boxes.append([class_id, x_center, y_center, width, height])\n",
    "\n",
    "            # 20 ô đầu là one-hot-encoding, 1 ô px, \n",
    "\n",
    "        return np.array(boxes)\n",
    "    \n",
    "# Class mapping \n",
    "class_mapping = {\n",
    "    'aeroplane': 0,\n",
    "    'bicycle': 1,\n",
    "    'bird': 2,\n",
    "    'boat': 3,\n",
    "    'bottle': 4,\n",
    "    'bus': 5,\n",
    "    'car': 6,\n",
    "    'cat': 7,\n",
    "    'chair': 8,\n",
    "    'cow': 9,\n",
    "    'diningtable': 10,\n",
    "    'dog': 11,\n",
    "    'horse': 12,\n",
    "    'motorbike': 13,\n",
    "    'person': 14,\n",
    "    'pottedplant': 15,\n",
    "    'sheep': 16,\n",
    "    'sofa': 17,\n",
    "    'train': 18,\n",
    "    'tvmonitor': 19\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e02ef5",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "debdede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 448\n",
    "HEIGHT = 448\n",
    "\n",
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.HueSaturationValue(\n",
    "                        hue_shift_limit=0.2,\n",
    "                        sat_shift_limit=0.2,\n",
    "                        val_shift_limit=0.2,\n",
    "                        p=0.9,\n",
    "                    ),\n",
    "                    A.RandomBrightnessContrast(\n",
    "                        brightness_limit=0.2,\n",
    "                        contrast_limit=0.2,\n",
    "                        p=0.9,\n",
    "                    ),\n",
    "                ],\n",
    "                p=0.9,\n",
    "            ),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.2),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.Resize(height=WIDTH, width=WIDTH, p=1.0),\n",
    "            # A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"yolo\",\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=[\"labels\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=WIDTH, width=WIDTH, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"yolo\",\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=[\"labels\"],\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936096b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[69, 66, 67,  ..., 55, 54, 51],\n",
       "         [71, 67, 67,  ..., 55, 52, 52],\n",
       "         [69, 68, 67,  ..., 57, 54, 55],\n",
       "         ...,\n",
       "         [79, 80, 80,  ..., 80, 81, 81],\n",
       "         [81, 82, 82,  ..., 79, 79, 79],\n",
       "         [82, 83, 83,  ..., 80, 79, 79]],\n",
       "\n",
       "        [[69, 66, 67,  ..., 56, 56, 53],\n",
       "         [71, 67, 67,  ..., 56, 54, 54],\n",
       "         [69, 68, 67,  ..., 58, 56, 57],\n",
       "         ...,\n",
       "         [78, 79, 80,  ..., 80, 81, 81],\n",
       "         [80, 81, 81,  ..., 79, 79, 79],\n",
       "         [81, 82, 81,  ..., 81, 79, 79]],\n",
       "\n",
       "        [[67, 64, 65,  ..., 56, 55, 52],\n",
       "         [68, 65, 65,  ..., 55, 53, 53],\n",
       "         [67, 66, 65,  ..., 57, 55, 56],\n",
       "         ...,\n",
       "         [74, 75, 77,  ..., 78, 79, 79],\n",
       "         [76, 77, 79,  ..., 76, 77, 77],\n",
       "         [77, 78, 79,  ..., 76, 77, 77]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tạo dataset\n",
    "dataset = CustomVOCDataset(\n",
    "    root='./data',  # Đường dẫn tới thư mục VOC\n",
    "    year='2007',  # Phiên bản VOC\n",
    "    image_set='train',\n",
    "    download=False,\n",
    "    class_mapping=class_mapping,\n",
    "    custom_transforms=get_train_transforms\n",
    ")\n",
    "\n",
    "# Lấy một mẫu\n",
    "image, labels_matrix = dataset.__getitem__(0)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 650614,
     "sourceId": 1151625,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5617.693429,
   "end_time": "2025-01-19T13:23:12.461327",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-19T11:49:34.767898",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
